{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa2f77b-5827-4024-bc5c-e27770812047",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:56.402837Z",
     "end_time": "2023-04-05T19:35:56.435543Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0063ebed-93e1-4927-9052-879e5d3dcf90",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:56.675408Z",
     "end_time": "2023-04-05T19:35:59.214928Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy.optimize\n",
    "\n",
    "from models import *\n",
    "from replay_memory import Memory\n",
    "from running_state import ZFilter\n",
    "from trpo import trpo_step\n",
    "from utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.utils.backcompat.broadcast_warning.enabled = True\n",
    "torch.utils.backcompat.keepdim_warning.enabled = True\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_trpo_tar_mean_adv(rewards, masks, actions, values):\n",
    "    returns = torch.Tensor(actions.size(0),1)\n",
    "    deltas = torch.Tensor(actions.size(0),1)\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_return = 0\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        returns[i] = rewards[i] + gamma * prev_return * masks[i]\n",
    "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values.data[i]\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_return = returns[i, 0]\n",
    "        prev_value = values.data[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    targets = returns\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    return targets, advantages\n",
    "\n",
    "def get_atrpo_tar_mean_adv(rewards, masks, actions, values):\n",
    "    ro = torch.mean(rewards)\n",
    "\n",
    "    deltas_ = torch.Tensor(actions.size(0),1)\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "    targets = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        targets[i] = rewards[i] - ro + prev_value * masks[i]\n",
    "        deltas_[i] = rewards[i] - ro + prev_value * masks[i] - values.data[i]\n",
    "        advantages[i] = deltas_[i] + tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_value = values.data[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    return targets, advantages\n",
    "\n",
    "def get_atrpo_tar_no_mean_adv(rewards, masks, actions, values):\n",
    "    ro = torch.mean(rewards)\n",
    "\n",
    "    deltas_ = torch.Tensor(actions.size(0),1)\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "    targets = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        targets[i] = rewards[i] - ro + prev_value * masks[i]\n",
    "        deltas_[i] = rewards[i] - ro + prev_value * masks[i] - values.data[i]\n",
    "        advantages[i] = deltas_[i] + tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_value = values.data[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    # advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    return targets, advantages\n",
    "\n",
    "\n",
    "def get_atrpo_tar_adv_0(rewards, masks, actions, values):\n",
    "    ro = torch.mean(rewards)\n",
    "\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "    targets = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_value = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        targets[i] = rewards[i] - ro + prev_value * masks[i]\n",
    "        advantages[i] = rewards[i] - ro + prev_value * masks[i] - values.data[i]\n",
    "\n",
    "        prev_value = values.data[i, 0]\n",
    "\n",
    "    return targets, advantages\n",
    "\n",
    "def get_atrpo_tar_adv_1(rewards, masks, actions, values):\n",
    "    ro = torch.mean(rewards)\n",
    "\n",
    "    advantages = torch.Tensor(actions.size(0),1)\n",
    "    targets = torch.Tensor(actions.size(0),1)\n",
    "\n",
    "    prev_value = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        targets[i] = rewards[i] - ro + prev_value\n",
    "        advantages[i] = rewards[i] - ro + prev_value - values.data[i]\n",
    "\n",
    "        prev_value = values.data[i, 0]\n",
    "\n",
    "    return targets, advantages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:59.204768Z",
     "end_time": "2023-04-05T19:35:59.226653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f3a26b-262c-41b1-8da3-c0b3561a888b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:59.233365Z",
     "end_time": "2023-04-05T19:35:59.271726Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(batch, targ_adv_fun):\n",
    "    rewards = torch.tensor(np.array(batch.reward))\n",
    "    masks = torch.tensor(np.array(batch.mask))\n",
    "    actions = torch.Tensor(np.concatenate(batch.action, 0)).detach()\n",
    "    states = torch.tensor(np.array(batch.state)).detach()\n",
    "    values = val(states)\n",
    "\n",
    "    targets, advantages = targ_adv_fun(rewards, masks, actions, values)\n",
    "\n",
    "    targets = targets.detach()\n",
    "    advantages = advantages.detach()\n",
    "\n",
    "    # Original code uses the same LBFGS to optimize the value loss\n",
    "    def get_value_loss(flat_params):\n",
    "        set_flat_params_to(val, torch.Tensor(flat_params))\n",
    "        for param in val.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.fill_(0)\n",
    "\n",
    "        value_loss = (val(states) - targets).pow(2).mean()\n",
    "\n",
    "        # weight decay\n",
    "        for param in val.parameters():\n",
    "            value_loss += param.pow(2).sum() * l2_reg\n",
    "        value_loss.backward()\n",
    "        return (value_loss.data.double().numpy(), get_flat_grad_from(val).data.double().numpy())\n",
    "\n",
    "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss, get_flat_params_from(val).double().numpy(), maxiter=25)\n",
    "    set_flat_params_to(val, torch.Tensor(flat_params))\n",
    "\n",
    "    action_means, action_log_stds, action_stds = agent(states)\n",
    "    fixed_log_prob = normal_log_density(actions, action_means, action_log_stds, action_stds).data.clone().detach()\n",
    "\n",
    "    def get_loss(volatile=False):\n",
    "        if volatile:\n",
    "            with torch.no_grad():\n",
    "                action_means, action_log_stds, action_stds = agent(states)\n",
    "        else:\n",
    "            action_means, action_log_stds, action_stds = agent(states)\n",
    "                \n",
    "        log_prob = normal_log_density(actions, action_means, action_log_stds, action_stds)\n",
    "        action_loss = -(advantages * torch.exp(log_prob - fixed_log_prob))\n",
    "        return action_loss.mean()\n",
    "\n",
    "\n",
    "    def get_kl():\n",
    "        mean1, log_std1, std1 = agent(states)\n",
    "\n",
    "        mean0 = mean1.data.detach()\n",
    "        log_std0 = log_std1.data.detach()\n",
    "        std0 = std1.data.detach()\n",
    "        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    trpo_step(agent, get_loss, get_kl, max_kl, damping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "env_name = \"Humanoid-v4\"#\"HalfCheetah-v4\"\n",
    "seed = 167\n",
    "gamma = 0.99\n",
    "tau = 0.97\n",
    "l2_reg = 1e-2\n",
    "max_kl = 1e-2\n",
    "damping = 1e-1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:59.273303Z",
     "end_time": "2023-04-05T19:35:59.308582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 500x300 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get_trpo_tar_mean_adv - trpo with normed advantege\n",
    "#get_atrpo_tar_mean_adv - atrpo with normed advantege (Bad)\n",
    "#get_atrpo_tar_adv_0 - atrpo with masks\n",
    "#get_atrpo_tar_adv_1 - atrpo without masks\n",
    "\n",
    "render = False\n",
    "compare_plots = True\n",
    "plt.figure(figsize=(5,3))\n",
    "targ_adv_funs = [get_trpo_tar_mean_adv, get_atrpo_tar_no_mean_adv, get_atrpo_tar_adv_0]\n",
    "\n",
    "ext_its = 3 #num of experiments\n",
    "i_episodes = 5 # number of batches in an experiment\n",
    "\n",
    "t_steps = 1000 # path len\n",
    "batch_size = 15000 # t_steps * (?)\n",
    "log_interval = max(1,i_episodes//10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:59.306741Z",
     "end_time": "2023-04-05T19:35:59.347360Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "targ_adv_funs_len = len(targ_adv_funs)\n",
    "all_acc_batch_rewards = [[] for i in range(len(targ_adv_funs))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:35:59.336023Z",
     "end_time": "2023-04-05T19:35:59.390533Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d52dc3-94c7-4fc2-9cde-aef0ec24ad96",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-04-03T19:47:37.348340Z",
     "end_time": "2023-04-04T02:06:03.136485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/home/a-kholodov/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001B[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/home/a-kholodov/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001B[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001B[0m\n",
      "  deprecation(\n",
      "/home/a-kholodov/.local/lib/python3.8/site-packages/gym/core.py:256: DeprecationWarning: \u001B[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001B[0m\n",
      "  deprecation(\n",
      "/home/a-kholodov/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tFunc name: get_trpo_tar_mean_adv\tAverage reward -1649.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 30\u001B[0m\n\u001B[1;32m     28\u001B[0m reward_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(t_steps):\n\u001B[0;32m---> 30\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     action \u001B[38;5;241m=\u001B[39m action\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     32\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m~/Desktop/diploma_code/models.py:28\u001B[0m, in \u001B[0;36mATRPOAgent.act\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mact\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 28\u001B[0m         action_mean, _, action_std \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m         action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnormal(action_mean, action_std)\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m action\n",
      "File \u001B[0;32m~/Desktop/diploma_code/models.py:18\u001B[0m, in \u001B[0;36mATRPOAgent.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     action_mean \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_mean(x)\n\u001B[1;32m     21\u001B[0m     action_log_std \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_std\u001B[38;5;241m.\u001B[39mexpand_as(action_mean)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for ext_it in tqdm(range(ext_its * targ_adv_funs_len)):\n",
    "    cur_exp_id = ext_it // targ_adv_funs_len\n",
    "    cur_f_id = ext_it % targ_adv_funs_len\n",
    "    cur_targ_adv_func = targ_adv_funs[cur_f_id]\n",
    "    cur_seed = seed + cur_exp_id\n",
    "\n",
    "    env = gym.make(env_name, terminate_when_unhealthy = False)\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "    env.seed(cur_seed)\n",
    "    torch.manual_seed(cur_seed)\n",
    "    agent = ATRPOAgent(num_inputs, num_actions)\n",
    "    val = Value(num_inputs)\n",
    "    running_state = ZFilter((num_inputs,), clip=5)\n",
    "    running_reward = ZFilter((1,), demean=False, clip=10)\n",
    "    acc_batch_rewards = []\n",
    "\n",
    "    for i_episode in range(i_episodes):\n",
    "        memory = Memory()\n",
    "\n",
    "        num_steps = 0\n",
    "        reward_batch = 0\n",
    "        num_episodes = 0\n",
    "        while num_steps < batch_size:\n",
    "            state = env.reset()\n",
    "            state = running_state(state)\n",
    "\n",
    "            reward_sum = 0\n",
    "            for t in range(t_steps):\n",
    "                action = agent.act(state)\n",
    "                action = action.data[0].numpy()\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                reward_sum += reward\n",
    "\n",
    "                next_state = running_state(next_state)\n",
    "\n",
    "                mask = 0 if done else 1\n",
    "\n",
    "                memory.push(state, np.array([action]), mask, next_state, reward)\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "            num_steps += (t-1)\n",
    "            num_episodes += 1\n",
    "            reward_batch += reward_sum\n",
    "\n",
    "        reward_batch /= num_episodes\n",
    "        acc_batch_rewards.append(reward_batch)\n",
    "        batch = memory.sample()\n",
    "        update_params(batch,cur_targ_adv_func)\n",
    "\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tFunc name: {}\\tAverage reward {:.2f}'.format(\n",
    "                i_episode, cur_targ_adv_func.__name__, reward_batch))\n",
    "\n",
    "    plt.title(cur_targ_adv_func.__name__)\n",
    "\n",
    "    plt.plot(acc_batch_rewards)\n",
    "    plt.show()\n",
    "\n",
    "    all_acc_batch_rewards[cur_f_id].append(acc_batch_rewards)\n",
    "\n",
    "    if targ_adv_funs_len > 1 and compare_plots and cur_f_id == targ_adv_funs_len-1:\n",
    "        plt.title(cur_exp_id)\n",
    "        for i in range(targ_adv_funs_len):\n",
    "            plt.plot(all_acc_batch_rewards[i][-1],label=targ_adv_funs[i].__name__)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_array(fname, data):\n",
    "    with open(fname, 'w') as f:\n",
    "        print(data, file=f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:06:51.507869Z",
     "end_time": "2023-04-05T19:06:51.522397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_array('data/Human_Trpo_AtpoDelta_Atrpo0_100_0.txt', all_acc_batch_rewards)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-05T19:06:51.751458Z",
     "end_time": "2023-04-05T19:06:51.758471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_acc_batch_rewards"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-04T11:59:46.654687Z",
     "end_time": "2023-04-04T11:59:46.670279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
